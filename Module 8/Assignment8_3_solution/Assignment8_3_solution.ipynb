{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e28789e4ae42e5d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Required Assignment 8.3: Evaluating Multiple Models\n",
    "\n",
    "**Estimated Time: 120 Minutes**\n",
    "\n",
    "**Total points: 100**\n",
    "\n",
    "This assignment focuses on solving a specific regression problem using basic cross-validation with a train/test/validation split.  In addition to using the methods explored, this assignment also aims to familiarize you with further utilities for data transformation, including the `OneHotEncoder` and `OrdinalEncoder` along with their use in a `make_column_transformer`.  \n",
    "\n",
    "The operations of encoding categorical features will be introduced using `sklearn`.  This will allow you to streamline your model-building pipelines.  Depending on whether a string type feature is **ordinal** or **categorical** we want to encode differently.  The `OrdinalEncoder` will be used to encode features that do not need to be binarized due to an underlying order, and `OneHotEncoder` for categorical features (as a similar approach to that of the `.get_dummies()` method in pandas).  By the end of the assignment, you will see how to chain multiple feature encoding methods together, including the earlier `PolynomialFeatures` for numeric features. \n",
    "\n",
    "<center>\n",
    "    <img src = images/pipes.png width = 50% />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5c9b6ac6e449ecfc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Index\n",
    "\n",
    "- [Problem 1](#Problem-1)\n",
    "- [Problem 2](#Problem-2)\n",
    "- [Problem 3](#Problem-3)\n",
    "- [Problem 4](#Problem-4)\n",
    "- [Problem 5](#Problem-5)\n",
    "- [Problem 6](#Problem-6)\n",
    "- [Problem 7](#Problem-7)\n",
    "- [Problem 8](#Problem-8)\n",
    "- [Problem 9](#Problem-9)\n",
    "- [Problem 10](#Problem-10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\") #setting this will display your pipelines as seen above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f26ce8dce8a28e98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The Data: Ames Housing\n",
    "\n",
    "This dataset is a popular beginning dataset used in teaching regression.  The task is to use specific features of houses to predict the price of the house.  In addition to this, as discussed in video 8.10 -- this dataset is available for use in an ongoing competition where you can use the `test.csv` to submit your models predictions.  Accordingly, the two data files are identical with the exception of the `test.csv` file not containing the target feature.\n",
    "\n",
    "The data contains 81 columns of different information on the individual houses and their sale price.  A full description of the data is attached [here](data/data_description.txt).  In this assignment, you will use a small subset of the features to begin modeling with that includes ordinal, categorical, and numeric features. As an optional exercise, you are encouraged to continue engineering additional features and attempt to improve the performance of your model, including submitting the predictions on Kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note the difference in one column from train to test\n",
    "[i for i in train.columns if i not in test.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5fc95e37458a0411",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 1\n",
    "\n",
    "#### Train/Test split\n",
    "\n",
    "**5 Points**\n",
    "\n",
    "Despite having a test dataset, you want to create a holdout set to assess your models' performance.  To do so, use sklearn's `train_test_split` to split `X` and `y` with arguments:\n",
    "\n",
    "- `test_size = 0.3`\n",
    "- `random_state = 22`\n",
    "\n",
    "Assign your results to `X_train, X_test, y_train, y_test`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('SalePrice', axis = 1)\n",
    "y = train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8fb6e1fcf3d2671",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "X_train, X_test, y_train, y_test = '', '', '', ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22)\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(type(X_train), type(y_train))#should be DataFrame and Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4a49dcddc30c0ab0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 2\n",
    "\n",
    "#### Baseline Predictions\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Before building a regression model, you should set a baseline to compare your later models to.  One way to do this is to guess the mean of the `SalePrice` column.  For the variables `baseline_train` and `baseline_test`, create arrays of same shape as `y_train` and `y_test` respectively.  The variable `baseline_train` should contain `y_train.mean()`. The variable `baseline_test` should contain `y_test.mean()`.\n",
    "\n",
    "\n",
    "Use the  `mean_squared_error` function to calculate the error between `baseline_train` and `y_train`, Assign the result to `mse_baseline_train`.\n",
    "\n",
    "Use the  `mean_squared_error` function to calculate the error between `baseline_test` and `y_test`, Assign the result to `mse_baseline_test`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5cbc5d640d56621c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "baseline_train = ''\n",
    "baseline_test = ''\n",
    "mse_baseline_train = ''\n",
    "mse_baseline_test = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "baseline_train = np.ones(shape = y_train.shape)*y_train.mean()\n",
    "baseline_test = np.ones(shape = y_test.shape)*y_test.mean()\n",
    "mse_baseline_train = mean_squared_error(baseline_train, y_train)\n",
    "mse_baseline_test = mean_squared_error(baseline_test, y_test)\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(baseline_train.shape, baseline_test.shape)\n",
    "print(f'Baseline for training data: {mse_baseline_train}')\n",
    "print(f'Baseline for testing data: {mse_baseline_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d2b544b45400b514",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 3\n",
    "\n",
    "#### Examining the Correlations\n",
    "\n",
    "**5 Points**\n",
    "\n",
    "What feature has the highest positive correlation with `SalePrice`?  Assign your answer as a string matching the column name exactly to `highest_corr` below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d7ab063f61943eb1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "highest_corr = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "#highest_corr = train.corr()[['SalePrice']].nlargest(columns = 'SalePrice', n = 2).index[1]\n",
    "numeric_train = train.select_dtypes(include=[np.number])\n",
    "highest_corr = numeric_train.corr()[['SalePrice']].nlargest(columns='SalePrice', n=2).index[1]\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(highest_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2b08dc981729b7be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 4\n",
    "\n",
    "#### Simple Model\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Complete the code below according to the instructions below:\n",
    "\n",
    "- Define a variable `X1` and assign to it the calues in the column `OverallQual`.\n",
    "- Instantiate a `LinearRegression` model and use the `fit` function to train it using `X1` and `y_train`. Assing your result to `lr`.\n",
    "- Use the  `mean_squared_error` function to calculate the error between `y_train` and `lr.predict(X1)`. Assign the result to `model_1_train_mse`.\n",
    "- Use the  `mean_squared_error` function to calculate the error between `y_test` and `lr.predict(X_test[['OverallQual']]`. Assign the result to `model_1_test_mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cec09812a0afd62b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "model_1_train_mse = ''\n",
    "model_1_test_mse = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "X1 = X_train[['OverallQual']]\n",
    "lr = LinearRegression().fit(X1, y_train)\n",
    "model_1_train_mse = mean_squared_error(y_train, lr.predict(X1))\n",
    "model_1_test_mse = mean_squared_error(y_test, lr.predict(X_test[['OverallQual']]))\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(f'Train MSE: {model_1_train_mse: .2f}')\n",
    "print(f'Test MSE: {model_1_test_mse: .2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba47d90de87508e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 5\n",
    "\n",
    "#### Using `OneHotEncoder`\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Similar to the `pd.get_dummies()` method earlier encountered, scikit-learn has a utility for encoding categorical features in the same way.  Below, the `OneHotEncoder` is demonstrated in the `CentralAir` column.  You are to use these results to build a model where the only feature is the `CentralAir` column.  Note the two arguments are used in the `OneHotEncoder`:\n",
    "\n",
    "- `sparse = False`: returns an array that we can investigate vs with `sparse = True` you are returned a sparse matrix -- a memory saving representation\n",
    "- `drop = if_binary`: returns a single column for any binary categories.  This avoids redundant features in our regression model.\n",
    "\n",
    "In the code cell below, instantiate a `LinearRegression` model and use the `fit` function to train it using `model_2_train` and `y_train`. Assing your result to `model_2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the features\n",
    "central_air_train = X_train[['CentralAir']]\n",
    "central_air_test = X_test[['CentralAir']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a categorical feature\n",
    "central_air_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate a OHE object\n",
    "#sparse = False returns an array so we can view\n",
    "ohe = OneHotEncoder(sparse_output = False, drop='if_binary')\n",
    "print(ohe.fit_transform(central_air_train)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_train = ohe.fit_transform(central_air_train)\n",
    "model_2_test = ohe.transform(central_air_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-190debe81fbe71c7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "model_2 = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "model_2 = LinearRegression().fit(model_2_train, y_train)\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(model_2.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-18a9cb5b0c662a92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "\n",
    "To build a model using both the `OverallQual` column and the `CentralAir` column, you could use the `OneHotEncoder` to transform `CentralAir`, and then concatenate the results back into a DataFrame or numpy array.  To streamline this process, the `make_column_transformer` can be used to separate specific columns for certain transformations.  Below, a `make_column_transformer` has been created for you to do just this.  \n",
    "\n",
    "\n",
    "The arguments are tuples of the form `(transformer, columns)` that specify a transformation to perform on the given column.  Further, the `remainder = passthrough` argument says to just pass the other columns through.  You are returned a numpy array with the `CentralAir` column binarized and concatenated to the `OverallQual` feature.\n",
    "\n",
    "\n",
    "For an example using the `make_column_transformer` see [here](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_transformer = make_column_transformer((OneHotEncoder(drop = 'if_binary'), ['CentralAir']), \n",
    "                                          remainder='passthrough',force_int_remainder_cols=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_transformer.fit_transform(X_train[['OverallQual', 'CentralAir']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-96e912368e23f0e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 6\n",
    "\n",
    "#### Using `make_column_transformer`\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "\n",
    "Complete the code below according to the instructions below:\n",
    "\n",
    "\n",
    "- Use `Pipeline` to create a pipeline object. Inside the pipeline object define a a tuple where the first element is a string identifier `col_transformer` and the second element is an instance of `col_transformer`. Inside the pipeline define another tuple where the first element is a string identifier `linreg`, and the second element is an instance of `LinearRegression`. Assign the pipeline object to the variable `pipe_1`.\n",
    "- Use the `fit` function on `pipe_1` to train your model on `X_train[['OverallQual', 'CentralAir']]` and `y_train`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2129005f7b1c0a66",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "pipe_1 = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "pipe_1 = Pipeline([('col_transformer', col_transformer), ('linreg', LinearRegression())])\n",
    "\n",
    "pipe_1.fit(X_train[['OverallQual', 'CentralAir']], y_train)\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(pipe_1.named_steps)#col_transformer and linreg should be keys\n",
    "pipe_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-28a3a3221b800c5f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "\n",
    "Not all columns warrant binarization as done on the `CentralAir` column.  For example, consider the `HeatingQC` feature -- representing the quality of the heating in the house.  From the data description the unique values are described as:\n",
    "\n",
    "```\n",
    "HeatingQC: Heating quality and condition\n",
    "\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tAverage/Typical\n",
    "       Fa\tFair\n",
    "       Po\tPoor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-49cb3865102cab23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "These are ordered values, and rather than binarizing them a numeric value representing the scale can be used.  For example, using a scale of 0 - 4 you may associate the categories with an order in a list from least to greatest as:\n",
    "\n",
    "```\n",
    "['Po', 'Fa', 'TA', 'Gd', 'Ex']\n",
    "```\n",
    "\n",
    "Creating an `OrdinalEncoder` with these categories will transform the `HeatingQC` feature mapping each category as\n",
    "\n",
    "```\n",
    "Po: 0\n",
    "Fa: 1\n",
    "TA: 2\n",
    "Gd: 3\n",
    "Ex: 4\n",
    "```\n",
    "\n",
    "This is demonstrated below, and in a similar manner the use of the `make_column_transformer` is shown using the three columns `['OverallQual', 'CentralAir', 'HeatingQC']`, applying the appropriate transformations to each column and passing the remaining numeric feature through.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe = OrdinalEncoder(categories = [['Po', 'Fa', 'TA', 'Gd', 'Ex']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe.fit_transform(X_train[['HeatingQC']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['HeatingQC'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_ohe_transformer = make_column_transformer((OneHotEncoder(drop = 'if_binary'), ['CentralAir']),\n",
    "                                          (OrdinalEncoder(categories = [['Po', 'Fa', 'TA', 'Gd', 'Ex']]), ['HeatingQC']),\n",
    "                                          remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_ohe_transformer.fit_transform(X_train[['OverallQual', 'CentralAir', 'HeatingQC']])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['OverallQual', 'CentralAir', 'HeatingQC']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-76851882f343f51e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 7\n",
    "\n",
    "#### Using `OrdinalEncoder`\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "\n",
    "Complete the code below according to the instructions below:\n",
    "\n",
    "\n",
    "- Use `Pipeline` to create a pipeline object. Inside the pipeline object define a a tuple where the first element is a string identifier `transformer` and the second element is an instance of `ordinal_ohe_transformer`. Inside the pipeline define another tuple where the first element is a string identifier `linreg`, and the second element is an instance of `LinearRegression`. Assign the pipeline object to the variable `pipe_2`.\n",
    "- Use the `fit` function on `pipe_2` to train your model on `X_train[['OverallQual', 'CentralAir', 'HeatingQC']]` and `y_train`. \n",
    "- Use the `predict` function on `pipe_2` to make your predictions of `X_train[['OverallQual', 'CentralAir', 'HeatingQC']]`. Assign the result to `pred_train`.\n",
    "- - Use the `predict` function on `pipe_2` to make your predictions of `X_test[['OverallQual', 'CentralAir', 'HeatingQC']]`. Assign the result to `pred_test`.\n",
    "- Use the `mean_squared_error` function to calculate the MSE between `y_train` and `pred_train`. Assign the result to `pipe_2_train_mse`.\n",
    "- Use the `mean_squared_error` function to calculate the MSE between `y_test` and `pred_test`. Assign the result to `pipe_2_test_mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e902a6f9f27546fd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "pipe_2 = ''\n",
    "pipe_2_train_mse = ''\n",
    "pipe_2_test_mse = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "pipe_2 = Pipeline([('transformer', ordinal_ohe_transformer), \n",
    "                  ('linreg', LinearRegression())])\n",
    "pipe_2.fit(X_train[['OverallQual', 'CentralAir', 'HeatingQC']], y_train)\n",
    "pred_train = pipe_2.predict(X_train[['OverallQual', 'CentralAir', 'HeatingQC']])\n",
    "pred_test = pipe_2.predict(X_test[['OverallQual', 'CentralAir', 'HeatingQC']])\n",
    "pipe_2_train_mse = mean_squared_error(y_train, pred_train)\n",
    "pipe_2_test_mse = mean_squared_error(y_test, pred_test)\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(pipe_2.named_steps)\n",
    "print(f'Train MSE: {pipe_2_train_mse: .2f}')\n",
    "print(f'Test MSE: {pipe_2_test_mse: .2f}')\n",
    "pipe_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d37ccef9c286dbd9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 8\n",
    "\n",
    "#### Including `PolynomialFeatures`\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Finally, the earlier transformation of continuous columns using the `PolynomialFeatures` with `degree = 2` can be implemented alongside the `OneHotEncoder` and `OrdinalEncoder`.  \n",
    "\n",
    "The `make_column_transformer` is again used, and you are to create a `Pipeline` with steps `transformer` and `linreg`.  \n",
    "\n",
    "The `Pipeline` is fit on the training data using features `['OverallQual', 'CentralAir', 'HeatingQC']`.  \n",
    "\n",
    "- Use the `predict` function on `pipe_3` to predict the values of `X_train[['OverallQual', 'CentralAir', 'HeatingQC']]`. Assign your result to `quad_train_preds`.\n",
    "- Use the `predict` function on `pipe_3` to predict the values of `X_test[['OverallQual', 'CentralAir', 'HeatingQC']]`. Assign your result to `quad_test_preds`.\n",
    "- Use the `mean_squared_error` function to calculate the MSE between `y_train` and `quad_train_preds`. Assign the result to `quad_train_mse`.\n",
    "- Use the `mean_squared_error` function to calculate the MSE between `y_test` and `quad_test_preds`. Assign the result to `quad_test_mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_ordinal_ohe = make_column_transformer((OrdinalEncoder(categories = [['Po', 'Fa', 'TA', 'Gd', 'Ex']]), ['HeatingQC']),\n",
    "                                           (OneHotEncoder(drop = 'if_binary'), ['CentralAir']),\n",
    "                                           (PolynomialFeatures(include_bias = False, degree = 2), ['OverallQual']))\n",
    "pipe_3 = Pipeline([('transformer', poly_ordinal_ohe), \n",
    "                  ('linreg', LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_3.fit(X_train[['OverallQual', 'CentralAir', 'HeatingQC']], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6166bd840763ffd6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "quad_train_mse = ''\n",
    "quad_test_mse = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "quad_train_preds = pipe_3.predict(X_train[['OverallQual', 'CentralAir', 'HeatingQC']])\n",
    "quad_test_preds = pipe_3.predict(X_test[['OverallQual', 'CentralAir', 'HeatingQC']])\n",
    "quad_train_mse = mean_squared_error(y_train, quad_train_preds)\n",
    "quad_test_mse = mean_squared_error(y_test, quad_test_preds)\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(f'Train MSE: {quad_train_mse: .2f}')\n",
    "print(f'Test MSE: {quad_test_mse: .2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b0a0f9ce2acfd960",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 9\n",
    "\n",
    "#### Including More Features\n",
    "\n",
    "**20 Points**\n",
    "\n",
    "Use the following features to build a new `make_column_transformer` and fit 5 different models of degree 1 - 5 using the `degree` argument in your `PolynomialFeatures` transformer.  Keep track of the subsequent train mean squared error and test set mean squared error with the lists `train_mses` and `test_mses` respectively.  \n",
    "\n",
    "The `poly_ordinal_ohe` object contains the different transformers needed.  Note that rather than passing a list of columns to the `PolynomialFeatures` transformer, the `make_column_selector` function is used to select any numeric feature.  For more information on the `make_column_selector` see [here](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_selector.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['CentralAir', 'HeatingQC', 'OverallQual', 'GrLivArea', 'KitchenQual', 'FullBath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_ordinal_ohe = make_column_transformer((PolynomialFeatures(), make_column_selector(dtype_include=np.number)),\n",
    "                                           (OrdinalEncoder(categories = [['Po', 'Fa', 'TA', 'Gd', 'Ex']]), ['HeatingQC', 'KitchenQual']),\n",
    "                                               (OneHotEncoder(drop = 'if_binary', sparse_output = False), ['CentralAir']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-77c2202178f5d8af",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "train_mses = []\n",
    "test_mses = []\n",
    "#for degree in 1 - 5\n",
    "for i in range(1, 6):\n",
    "    #create pipeline with PolynomialFeatures degree i \n",
    "    #ADD APPROPRIATE ARGUMENTS IN POLYNOMIALFEATURES\n",
    "    poly_ordinal_ohe = make_column_transformer((PolynomialFeatures(), make_column_selector(dtype_include=np.number)),\n",
    "                                           (OrdinalEncoder(categories = [['Po', 'Fa', 'TA', 'Gd', 'Ex']]), ['HeatingQC']),\n",
    "                                               (OneHotEncoder(drop = 'if_binary'), ['CentralAir']))\n",
    "    \n",
    "    \n",
    "    #fit on train\n",
    "\n",
    "    #predict on train and test\n",
    "\n",
    "    #compute mean squared errors\n",
    "    \n",
    "    #append to train_mses and test_mses respectively\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "train_mses = []\n",
    "test_mses = []\n",
    "#for degree in 1 - 5\n",
    "for i in range(1, 6):\n",
    "    #create pipeline with PolynomialFeatures degree i\n",
    "    poly_ordinal_ohe = make_column_transformer((PolynomialFeatures(degree = i), make_column_selector(dtype_include=np.number)),\n",
    "                                           (OrdinalEncoder(categories = [['Po', 'Fa', 'TA', 'Gd', 'Ex']]), ['HeatingQC']),\n",
    "                                               (OneHotEncoder(drop = 'if_binary'), ['CentralAir']))\n",
    "    pipe = Pipeline([('transformer', poly_ordinal_ohe), ('linreg', LinearRegression())])\n",
    "\n",
    "    pipe.fit(X_train[features], y_train)\n",
    "    #fit on train\n",
    "    p1 = pipe.predict(X_train[features])\n",
    "    p2 = pipe.predict(X_test[features])\n",
    "    #predict on train and test\n",
    "    train_mses.append(mean_squared_error(y_train, p1))\n",
    "    test_mses.append(mean_squared_error(y_test, p2))\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(train_mses)\n",
    "print(test_mses)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-71011b87c2040d72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 10\n",
    "\n",
    "#### Optimal Model Complexity \n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Based on your model's mean squared error on the testing data in **Problem 9** above, what was the optimal complexity?  Assign your answer as an integer to `best_complexity` below.  Compute the **MEAN SQUARED ERROR** of this model and assign it to `best_mse` as a float. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4fd87b41a5fe40d5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "best_complexity = ''\n",
    "best_mse = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "best_complexity = test_mses.index(min(test_mses)) + 1\n",
    "best_mse = min(test_mses)\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(f'The best degree polynomial model is:  {best_complexity}')\n",
    "print(f'The smallest mean squared error on the test data is : {best_mse: .2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f38feb0e2f015284",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Further Exploration\n",
    "\n",
    "This activity was meant to introduce you to a more streamlined modeling process using the `sklearn` library.  While your models should be performing better than the baseline, it is likely that with a bit more feature engineering and cross-validation, you would be able to further improve the performance.  You are encouraged to explore further feature engineering and encoding, particularly with handling missing values.  \n",
    "\n",
    "Additionally, other transformations on the data may be appropriate.  For example, if you look at the distribution of errors in your model, you will note that they are slightly skewed.  An assumption of a Linear Regression model is that these should be roughly normally distributed.  By building a model on the logarithm of the target column and evaluating the model on the logarithm of the testing data, you will improve towards this assumption.  Note that the actual kaggle exercise is judged on the **ROOT MEAN SQUARED ERROR** of the logarithm of the target feature. \n",
    "\n",
    "If interested, scikitlearn also provides a function `TransformedTargetRegressor` that will accomplish this transformation and can easily be added to a pipeline. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html) for more information on this transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
