{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5f0288b403ed1232",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Required Assignment 9.4: LASSO and Sequential Feature Selection\n",
    "\n",
    "\n",
    "**Expected Time: 60 Minutes**\n",
    "\n",
    "**Total Points: 50**\n",
    "\n",
    "This assignment introduces the `Ridge` regression estimator from scikitlearn.  You will revisit the insurance data from the previous assignment and experiment with varying the `alpha` parameter discussed in Video 9.4. Your work here is a basic introduction where complexity in the preprocessing steps will be added to scale your data.  For now, you are just to familiarize yourself with the `Ridge` regression estimator and its `alpha` parameter. \n",
    "\n",
    "This assignment compares a second regularized regression method -- the LASSO -- with that of sequential feature selection.  The LASSO will be briefly discussed below, and you will use the scikit learn implementation.  Rather than using the LASSO as a model, you are to compare it to the `SequentialFeatureSelection` transformer as a method to select important features for a regression model. \n",
    "\n",
    "\n",
    "#### Index\n",
    "\n",
    "- [Problem 1](#Problem-1)\n",
    "- [Problem 2](#Problem-2)\n",
    "- [Problem 3](#Problem-3)\n",
    "- [Problem 4](#Problem-4)\n",
    "- [Problem 5](#Problem-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, SelectFromModel\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e92bc69b9408cf26",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### The Data\n",
    "\n",
    "For this exercise, you will revisit the automotive data.  The goal is again to predict the `mpg` column using the other numeric features.  You will build a polynomial model of degree 3 to compare the results of a `Lasso` and that of a `LinearRegression` model. Finally, you will use the `Lasso` estimator to select features in a pipeline with `SelectFromModel`. \n",
    "\n",
    "Below, the train and test data is created for you as `auto_X_train`, `auto_X_test`, `auto_y_train`, and `auto_y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = pd.read_csv('data/auto.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate train/test data for auto\n",
    "auto_X = auto.drop(['mpg', 'name'], axis = 1)\n",
    "auto_y = auto['mpg']\n",
    "auto_X_train, auto_X_test, auto_y_train, auto_y_test = train_test_split(auto_X, auto_y, \n",
    "                                                                       test_size = 0.3,\n",
    "                                                                       random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-add79ff14f85c4b3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 1\n",
    "\n",
    "#### The auto data\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "To start, build a `Pipeline` named `auto_pipe` with named steps `polyfeatures`, `scaler` and `lasso` model that utilizes `PolynomialFeatures`, `StandardScaler`, and the `Lasso` estimator with the following parameters:\n",
    "\n",
    "- `degree = 3` in `PolynomialFeatures`\n",
    "- `include_bias = False` in `PolynomialFeatures`\n",
    "- `random_state = 42` in `Lasso`\n",
    "\n",
    "Fit the pipeline on `auto_X_train` and `auto_y_train` data given.  Extract the lasso coefficients from the pipeline and assign them as an array to `lasso_coefs` below.  \n",
    "\n",
    "**HINT**: Use the `.named_steps['lasso']` to extract that lasso estimator and use the `.coef_` attribute after fitting to access the model coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4b2351ce5efba3ad",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "auto_pipe = ''\n",
    "lasso_coefs = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "auto_pipe = Pipeline([('polyfeatures', PolynomialFeatures(degree = 3, include_bias = False)),\n",
    "                      ('scaler', StandardScaler()),\n",
    "                     ('lasso', Lasso(random_state = 42))])\n",
    "auto_pipe.fit(auto_X_train, auto_y_train)\n",
    "lasso_coefs = auto_pipe.named_steps['lasso'].coef_\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(type(lasso_coefs))\n",
    "print(lasso_coefs)\n",
    "auto_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-00d34c3ff5947d86",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 2\n",
    "\n",
    "#### Error in `Lasso` model\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Now, compute the mean squared error of the LASSO model on both the train and test data, `auto_X_train` and `auto_X_test`, respectively.  Assign this as a float to `lasso_train_mse` and `lasso_test_mse`, respectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-27bc7ac17713b3d7",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "lasso_train_mse = ''\n",
    "lasso_test_mse = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "lasso_train_mse = mean_squared_error(auto_y_train, auto_pipe.predict(auto_X_train))\n",
    "lasso_test_mse = mean_squared_error(auto_y_test, auto_pipe.predict(auto_X_test))\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(lasso_train_mse)\n",
    "print(lasso_test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6cd009f6888369f8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 3\n",
    "\n",
    "#### Non-zero coefficients\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Using the `lasso_coefs` determine the number of features with non-zero coefficients and determine the name of those features as a result of the polynomial feature transformation.  \n",
    "\n",
    "To do this, access the `named_steps['polyfeatures']` feature from the `auto_pipe` pipeline and chain the `get_feature_names_out()` to get the features name. Assign the result to `feature_names`.\n",
    "\n",
    "Next, create a DataFrame named `lasso_df` below that has two columns -- `feature` and `coef`.  To the `feature` column assign `feature_names`. To the `coef` column assign `lasso_coefs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e5ed130aa3396617",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "feature_names = ''\n",
    "lasso_df = ''\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "feature_names = auto_pipe.named_steps['polyfeatures'].get_feature_names_out()\n",
    "lasso_df = pd.DataFrame({'feature': feature_names, 'coef': lasso_coefs})\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(type(feature_names))\n",
    "lasso_df.loc[lasso_df['coef'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-af9586e34b8d17e2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 4\n",
    "\n",
    "#### Comparing `Lasso` to `SequentialFeatureSelection`\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "As seen above, the Lasso model effectively eliminated all but 6 features from the cubic polynomial example.  Now, you are to build a `Pipeline` object called `sequential_pipe` with named steps `poly_features`, `selector`, and `linreg` with `PolynomialFeatures`, `SequentialFeatureSelector`, and `LinearRegression` respectively that uses the folowing parameters:\n",
    "\n",
    "- `degree = 3` in `PolynomialFeatures` step `poly_features`\n",
    "- `include_bias = False` in `PolynomialFeatures` step `poly_features`\n",
    "- `n_features_to_select = 6` in `selector`\n",
    "\n",
    "Assign this pipeline object to `sequential_pipe`.\n",
    "\n",
    "Next, use the `fit` function on `scaled_pipe` to train your model on `auto_X_train` and `auto_y_train`. \n",
    "\n",
    "Use the `mean_squared_error` function to compute the MSE between `auto_y_train` and` sequential_pipe.predict(auto_X_train)`. Assign your result to `sequential_train_mse`.\n",
    "\n",
    "Use the `mean_squared_error` function to compute the MSE between `auto_y_test` and `sequential_pipe.predict(auto_X_test)`. Assign your result to `sequential_test_mse`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-113e81ee2e4d2705",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "sequential_pipe = ''\n",
    "sequential_train_mse = ''\n",
    "sequential_test_mse = ''\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "sequential_pipe = Pipeline([('poly_features', PolynomialFeatures(degree = 3, include_bias = False)),\n",
    "                           ('selector', SequentialFeatureSelector(LinearRegression(), \n",
    "                                                                  n_features_to_select=6)),\n",
    "                           ('linreg', LinearRegression())])\n",
    "sequential_pipe.fit(auto_X_train, auto_y_train)\n",
    "sequential_train_mse = mean_squared_error(auto_y_train, sequential_pipe.predict(auto_X_train))\n",
    "sequential_test_mse = mean_squared_error(auto_y_test, sequential_pipe.predict(auto_X_test))\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(sequential_train_mse)\n",
    "print(sequential_test_mse)\n",
    "sequential_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4b1766f9492c5fec",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 5\n",
    "\n",
    "#### Using `Lasso` as a feature selector\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Rather than using the `Lasso` as the estimator, you can use the results of the `Lasso` to select features that are subsequently used in a `LinearRegression` estimator.  To do so, scikitlearn provides a function in the `feature_selection` module called `SelectFromModel` that will select the features based on coefficients.  \n",
    "\n",
    "As such, using the `Lasso` estimator to select features would involve instantiating the `SelectFromModel` transformer and selecting features as:\n",
    "\n",
    "```python\n",
    "selector = SelectFromModel(Lasso())\n",
    "selector.transform(auto_X_train)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3406cfb052fec8d2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "From here, the selector can be used in a `Pipeline` after transforming the features and before building a regression model.  Such a pipeline is given below and you are to use this to fit on the training data and score on the testing data.  Which model performs better, the model with sequential feature selection or that using the `Lasso` to select the features?  \n",
    "\n",
    "Assign your train and test error using the `model_selector_pipe` as `selector_train_mse` and `selector_test_mse` below.\n",
    "\n",
    "For more information and examples on `SelectFromModel` see [here](https://scikit-learn.org/stable/modules/feature_selection.html#select-from-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selector_pipe = Pipeline([('poly_features', PolynomialFeatures(degree = 3, include_bias = False)),\n",
    "                                ('scaler', StandardScaler()),\n",
    "                                ('selector', SelectFromModel(Lasso())),\n",
    "                                    ('linreg', LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f36419f78eb4822a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "selector_train_mse = ''\n",
    "selector_test_mse = ''\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "model_selector_pipe.fit(auto_X_train, auto_y_train)\n",
    "selector_train_mse = mean_squared_error(auto_y_train, model_selector_pipe.predict(auto_X_train))\n",
    "selector_test_mse = mean_squared_error(auto_y_test, model_selector_pipe.predict(auto_X_test))\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(selector_train_mse)\n",
    "print(selector_test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b56b71b7f554b647",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Further work could involve grid searching parameters of both the transformers and estimators, as well as including a `Ridge` regressor in the mix.  For now, you should be getting comfortable using the scikitlearn `Pipeline` object to combine transformers and estimators. This module introduced examples that can mitigate overfitting in a regression context.  It is important to note that not one strategy is always best for a modeling problem.  Instead, you should consider multiple approaches and let your goals for the model guide you to determine which model best suits your performance metric."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
